{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "import torch\n",
        "import json\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "6_eIvniqa9OE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obJnXdprXS4R"
      },
      "outputs": [],
      "source": [
        "# Generate summary corpus\n",
        "\n",
        "# Load spaCy English language model for NLP processing\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
        "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "print(\"done\")\n",
        "# Load the Excel file\n",
        "file_path = '/content/drive/MyDrive/BTech_Project/data.xlsx'\n",
        "data = pd.read_excel(file_path)\n",
        "data = data.head(500)\n",
        "\n",
        "# Function to clean and prepare text for summarization\n",
        "def prepare_text(row):\n",
        "    text = f\"\"\"\n",
        "    {row['LinkedIn Name']} is currently working as {row['Description']} at {row['Organisation']}.\n",
        "    Based in {row['Location']}, they are part of the {row['Industry']} industry.\n",
        "    In their current role as {row['Current Role(s)']}, they have been with the company for {row['Tenure at Company']}.\n",
        "    Their background includes: {row['About']}.\n",
        "    \"\"\"\n",
        "    # Clean the text using NLP (removing stop words, punctuation, etc.)\n",
        "    doc = nlp(text)\n",
        "    cleaned_text = \" \".join([token.text for token in doc if not token.is_stop and not token.is_punct])\n",
        "    return cleaned_text\n",
        "\n",
        "# Function to generate a detailed and advanced summarized corpus using BART\n",
        "def generate_bart_summary(text):\n",
        "    inputs = bart_tokenizer(text, max_length=1024, return_tensors=\"pt\", truncation=True)\n",
        "    summary_ids = bart_model.generate(inputs[\"input_ids\"], num_beams=4, max_length=150, early_stopping=True)\n",
        "    summary = bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Function to create a summarized corpus for each person\n",
        "def generate_summarized_corpus(row):\n",
        "    text = prepare_text(row)\n",
        "    # Using BART to summarize the cleaned text\n",
        "    summary = generate_bart_summary(text)\n",
        "    return summary\n",
        "\n",
        "# Create a summarized corpus for each person in the dataset\n",
        "data['Corpus'] = data.apply(generate_summarized_corpus, axis=1)\n",
        "\n",
        "# Number of people (nodes)\n",
        "num_people = len(data)\n",
        "\n",
        "# Adjacency list to store connections\n",
        "adjacency_list = defaultdict(list)\n",
        "\n",
        "# Simulate connections using a normal distribution\n",
        "np.random.seed(42)\n",
        "connections_per_person = np.random.normal(loc=5, scale=2, size=num_people).astype(int)\n",
        "connections_per_person = np.clip(connections_per_person, 1, num_people - 1)  # Ensure valid number of connections\n",
        "\n",
        "# Generate adjacency lists\n",
        "for i, person in data.iterrows():\n",
        "    connections = np.random.choice(range(num_people), size=connections_per_person[i], replace=False)\n",
        "    for connection in connections:\n",
        "        if connection != i:  # Avoid self-loops\n",
        "            adjacency_list[person['LinkedIn Name']].append(data.iloc[connection]['LinkedIn Name'])\n",
        "\n",
        "# Displaying a sample of the summarized corpus and adjacency list\n",
        "sample_corpus = data[['LinkedIn Name', 'Corpus']].head(3)\n",
        "sample_adjacency_list = {k: adjacency_list[k] for k in list(adjacency_list.keys())[:3]}\n",
        "\n",
        "print(\"Sample Summarized Corpus:\")\n",
        "print(sample_corpus)\n",
        "\n",
        "print(\"\\nSample Adjacency List:\")\n",
        "for name, connections in sample_adjacency_list.items():\n",
        "    print(f\"{name} -> {', '.join(connections)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate adjacency list from generated summary corpus\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "# Load the pre-summarized corpus\n",
        "file_path = '/content/drive/MyDrive/BTech_Project/summarized_corpus.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Number of people (nodes)\n",
        "num_people = len(data)\n",
        "\n",
        "# Adjacency list to store connections\n",
        "adjacency_list = defaultdict(list)\n",
        "\n",
        "# Simulate connections using a normal distribution (15-20 connections per person)\n",
        "np.random.seed(42)\n",
        "connections_per_person = np.random.normal(loc=17.5, scale=2, size=num_people).astype(int)\n",
        "connections_per_person = np.clip(connections_per_person, 15, 20)  # Ensure between 15 and 20 connections\n",
        "\n",
        "# Generate adjacency lists\n",
        "for i, person in data.iterrows():\n",
        "    connections = np.random.choice(range(num_people), size=connections_per_person[i], replace=False)\n",
        "    for connection in connections:\n",
        "        if connection != i:  # Avoid self-loops\n",
        "            adjacency_list[person['LinkedIn Name']].append(data.iloc[connection]['LinkedIn Name'])\n",
        "\n",
        "# Display a sample of the adjacency list\n",
        "sample_adjacency_list = {k: adjacency_list[k] for k in list(adjacency_list.keys())[:3]}\n",
        "\n",
        "print(\"Sample Adjacency List:\")\n",
        "for name, connections in sample_adjacency_list.items():\n",
        "    print(f\"{name} -> {', '.join(connections)}\")\n",
        "\n",
        "# Save the adjacency list to a file (if needed)\n",
        "# For example, saving as JSON for further analysis\n",
        "import json\n",
        "with open('/content/drive/MyDrive/BTech_Project/adjacency_list.json', 'w') as f:\n",
        "    json.dump(adjacency_list, f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hd9Pv4WwbRwJ",
        "outputId": "2d086a46-a87d-4ce3-984c-8919dc0195c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Adjacency List:\n",
            "Saurabh Gupta -> Rachna Sharma, Prabakaran Pandian, Pawan Sut Sharma, Aayush Jha, Denis CA de Souza, Kartikay Garg, Aayush Garg, Kapil Kumar Narula, Ratnadeep Pawar, Chhaya Bhanti, Urv Patel, ANIL KUMAR SAMINENI, Mahadeva swamy, Karan Vyas, Shadab Ghazaly, Nitesh Singh, Channa Ghosh, chinmay Khanolkar\n",
            "Jatin Singh -> Jacob Lallawmsang, Suchit Dekivadia, Renuka Nair, Elango Sidhan, Naveen Verma, Vipin Kumar Yadav, Mahadeva swamy, Saiprasad Bhartu, Balaram Puttaiah, Rajpal Navalkar, Wilma Rodrigues, Rajat Parikh, Cherish Tota, Gurjot Singh, Dr.Rathin Sharma, Jayavardhan Shetty, Akash Kumar\n",
            "Nilesh Bhattad -> Jacob Lallawmsang, Narendra Patel, Ishant Sharma, Amit Saha, Anju Sasikumar, Dr Dnyaneshwar Battalwar, Shekar Prabhakar, Abde Ali Shabbir. ., DEEPAK KUMAR PANI, Suchit Dekivadia, Divyesh Chandera, Aayush Garg, Roopesh Rai, Sekhar C, Jayavardhan Shetty, Param Desai, Krishnan Komandur, Shibabrata Bhattacharjee\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate adjacency list with profession\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import json\n",
        "\n",
        "# Load the data for names and professions\n",
        "file_path = '/content/drive/MyDrive/BTech_Project/data.xlsx'  # Replace with your Excel file path\n",
        "profession_data = pd.read_excel(file_path)\n",
        "\n",
        "# Load the pre-summarized corpus for network\n",
        "summarized_corpus_path = '/content/drive/MyDrive/BTech_Project/summarized_corpus.csv'  # Replace with your corpus file path\n",
        "data = pd.read_csv(summarized_corpus_path)\n",
        "\n",
        "# Number of people (nodes)\n",
        "num_people = len(data)\n",
        "\n",
        "# Create a dictionary for professions (to map name to profession)\n",
        "name_to_profession = dict(zip(profession_data['LinkedIn Name'], profession_data['Description']))\n",
        "\n",
        "# Adjacency list to store connections with professions\n",
        "adjacency_list = defaultdict(list)\n",
        "\n",
        "# Simulate connections using a normal distribution (15-20 connections per person)\n",
        "np.random.seed(42)\n",
        "connections_per_person = np.random.normal(loc=17.5, scale=2, size=num_people).astype(int)\n",
        "connections_per_person = np.clip(connections_per_person, 15, 20)  # Ensure between 15 and 20 connections\n",
        "\n",
        "# Generate adjacency lists with names and professions\n",
        "for i, person in data.iterrows():\n",
        "    connections = np.random.choice(range(num_people), size=connections_per_person[i], replace=False)\n",
        "    for connection in connections:\n",
        "        if connection != i:  # Avoid self-loops\n",
        "            connected_person_name = data.iloc[connection]['LinkedIn Name']\n",
        "            connected_person_profession = name_to_profession.get(connected_person_name, 'Unknown')\n",
        "            adjacency_list[person['LinkedIn Name']].append((connected_person_name, connected_person_profession))\n",
        "\n",
        "# Save the adjacency list to a file (JSON format)\n",
        "adjacency_list_file = '/content/drive/MyDrive/BTech_Project/adjacency_list.json'  # Replace with your desired path\n",
        "with open(adjacency_list_file, 'w') as f:\n",
        "    json.dump(adjacency_list, f)\n",
        "\n",
        "print(f\"Adjacency list saved to {adjacency_list_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_iNhFL5-MJ6",
        "outputId": "4b4bc771-0bce-497c-96ea-16290b1d78d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjacency list saved to /content/drive/MyDrive/BTech_Project/adjacency_list.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DFS\n",
        "\n",
        "def dfs_with_path(start, profession, visited=None, path=None):\n",
        "    if visited is None:\n",
        "        visited = set()\n",
        "    if path is None:\n",
        "        path = [start]\n",
        "\n",
        "    visited.add(start)\n",
        "\n",
        "    # Check if the current person has the required profession\n",
        "    if name_to_profession.get(start, '') == profession:\n",
        "        return path  # Return the path to the person with the required profession\n",
        "\n",
        "    # Perform DFS on the neighbors\n",
        "    for neighbor, _ in adjacency_list.get(start, []):\n",
        "        if neighbor not in visited:\n",
        "            result = dfs_with_path(neighbor, profession, visited, path + [neighbor])\n",
        "            if result:\n",
        "                return result  # Return the path as soon as a person with the required profession is found\n",
        "\n",
        "    return None  # No person with the required profession found in the network\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rthn_xpK_bdT",
        "outputId": "0725fa36-f680-49c9-b2b1-68faa247f55a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saurabh Gupta -> Rachna Sharma -> Rain Ramesh Babu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BFS\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "def bfs_with_path(start, profession):\n",
        "    # Queue for BFS (stores (name, path))\n",
        "    queue = deque([(start, [start])])\n",
        "    visited = set()\n",
        "    visited.add(start)\n",
        "\n",
        "    while queue:\n",
        "        current, path = queue.popleft()\n",
        "\n",
        "        # Check if the current person has the required profession\n",
        "        if name_to_profession.get(current, '') == profession:\n",
        "            return path  # Return the path to the person with the required profession\n",
        "\n",
        "        # Explore the neighbors (connections)\n",
        "        for neighbor, _ in adjacency_list.get(current, []):\n",
        "            if neighbor not in visited:\n",
        "                visited.add(neighbor)\n",
        "                queue.append((neighbor, path + [neighbor]))\n",
        "\n",
        "    return None  # No person with the required profession found\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qejwwrorCj8q",
        "outputId": "33c4456f-b760-49a7-f388-02d4ddc271aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saurabh Gupta -> Kartikay Garg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "starting_name = \"Saurabh Gupta\"\n",
        "required_profession = \"Chief Executive Officer\"\n",
        "\n",
        "# path = dfs_with_path(starting_name, required_profession)\n",
        "path = bfs_with_path(starting_name, required_profession)\n",
        "\n",
        "if path:\n",
        "    print(\" -> \".join(path))  # Print the path\n",
        "else:\n",
        "    print(f\"No person with the profession {required_profession} found in the network starting from {starting_name}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZiCLFDjDOtk",
        "outputId": "f98df9a5-8e24-4da8-fe4f-c3d544949037"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saurabh Gupta -> Kartikay Garg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "starting_name = \"Saurabh Gupta\"\n",
        "required_profession = \"Chief Executive Officer\"\n",
        "\n",
        "# path = dfs_with_path(starting_name, required_profession)\n",
        "path = bfs_with_path(starting_name, required_profession)\n",
        "\n",
        "if path:\n",
        "    print(\" -> \".join(path))  # Print the path\n",
        "\n",
        "    # Extract the last person (the one with the required profession) from the path\n",
        "    selected_person = path[-1]\n",
        "\n",
        "    # Load the summarized corpus data\n",
        "    file_path = '/content/drive/MyDrive/BTech_Project/summarized_corpus.csv'  # Path to the summarized corpus file\n",
        "    with open(file_path, 'r') as f:\n",
        "        corpus_data = f.readlines()\n",
        "\n",
        "    # Find the row with the selected person\n",
        "    person_data = None\n",
        "    for line in corpus_data:\n",
        "        name, data = line.split(\",\", 1)  # Split the line by the first comma\n",
        "        if name.strip() == selected_person:  # Compare the name with the selected person\n",
        "            person_data = data.strip()  # Get the corpus data (remove leading/trailing spaces)\n",
        "            break\n",
        "\n",
        "    if person_data:\n",
        "        print(\"\\nCorpus data for selected person:\")\n",
        "        print(person_data)\n",
        "    else:\n",
        "        print(f\"No corpus data found for {selected_person}\")\n",
        "else:\n",
        "    print(f\"No person with the profession {required_profession} found in the network starting from {starting_name}.\")\n"
      ],
      "metadata": {
        "id": "OGUqWgf4FbRo",
        "outputId": "89f0ee7d-4179-4438-f614-bf515e5d8028",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saurabh Gupta -> Kartikay Garg\n",
            "\n",
            "Corpus data for selected person:\n",
            "\"Kartikay Garg is the Chief Executive Officer of Recycle City in India. His background includes nan, nanotechnology and recycling. He is a graduate of the Indian Institute of Technology, Kharagpur. He has worked in the recycling industry for more than a decade.\"\n"
          ]
        }
      ]
    }
  ]
}