{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shant\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "import torch\n",
    "import json\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary corpus\n",
    "\n",
    "# Load spaCy English language model for NLP processing\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Load BART model and tokenizer for summarization\n",
    "bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "print(\"done\")\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = '/content/drive/MyDrive/BTech_Project/data.xlsx'\n",
    "data = pd.read_excel(file_path)\n",
    "data = data.head(500)\n",
    "\n",
    "# Function to clean and prepare text for summarization\n",
    "def prepare_text(row):\n",
    "    text = f\"\"\"\n",
    "    {row['LinkedIn Name']} is currently working as {row['Description']} at {row['Organisation']}.\n",
    "    Based in {row['Location']}, they are part of the {row['Industry']} industry.\n",
    "    In their current role as {row['Current Role(s)']}, they have been with the company for {row['Tenure at Company']}.\n",
    "    Their background includes: {row['About']}.\n",
    "    \"\"\"\n",
    "    # Clean the text using NLP (removing stop words, punctuation, etc.)\n",
    "    doc = nlp(text)\n",
    "    cleaned_text = \" \".join([token.text for token in doc if not token.is_stop and not token.is_punct])\n",
    "    return cleaned_text\n",
    "\n",
    "# Function to generate a detailed and advanced summarized corpus using BART\n",
    "def generate_bart_summary(text):\n",
    "    inputs = bart_tokenizer(text, max_length=1024, return_tensors=\"pt\", truncation=True)\n",
    "    summary_ids = bart_model.generate(inputs[\"input_ids\"], num_beams=4, max_length=150, early_stopping=True)\n",
    "    summary = bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# Function to create a summarized corpus for each person\n",
    "def generate_summarized_corpus(row):\n",
    "    text = prepare_text(row)\n",
    "    # Using BART to summarize the cleaned text\n",
    "    summary = generate_bart_summary(text)\n",
    "    return summary\n",
    "\n",
    "# Create a summarized corpus for each person in the dataset\n",
    "data['Corpus'] = data.apply(generate_summarized_corpus, axis=1)\n",
    "\n",
    "# Save the summarized corpus to a CSV file\n",
    "output_file_path = './summarized_corpus.csv'\n",
    "data[['LinkedIn Name', 'Corpus']].to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Summarized corpus saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Adjacency List:\n",
      "Saurabh Gupta -> Rachna Sharma, Prabakaran Pandian, Pawan Sut Sharma, Aayush Jha, Denis CA de Souza, Kartikay Garg, Aayush Garg, Kapil Kumar Narula, Ratnadeep Pawar, Chhaya Bhanti, Urv Patel, ANIL KUMAR SAMINENI, Mahadeva swamy, Karan Vyas, Shadab Ghazaly, Nitesh Singh, Channa Ghosh, chinmay Khanolkar\n",
      "Jatin Singh -> Jacob Lallawmsang, Suchit Dekivadia, Renuka Nair, Elango Sidhan, Naveen Verma, Vipin Kumar Yadav, Mahadeva swamy, Saiprasad Bhartu, Balaram Puttaiah, Rajpal Navalkar, Wilma Rodrigues, Rajat Parikh, Cherish Tota, Gurjot Singh, Dr.Rathin Sharma, Jayavardhan Shetty, Akash Kumar\n",
      "Nilesh Bhattad -> Jacob Lallawmsang, Narendra Patel, Ishant Sharma, Amit Saha, Anju Sasikumar, Dr Dnyaneshwar Battalwar, Shekar Prabhakar, Abde Ali Shabbir. ., DEEPAK KUMAR PANI, Suchit Dekivadia, Divyesh Chandera, Aayush Garg, Roopesh Rai, Sekhar C, Jayavardhan Shetty, Param Desai, Krishnan Komandur, Shibabrata Bhattacharjee\n"
     ]
    }
   ],
   "source": [
    "# Generate adjacency list from generated summary corpus\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load the pre-summarized corpus\n",
    "file_path = './summarized_corpus.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Number of people (nodes)\n",
    "num_people = len(data)\n",
    "\n",
    "# Adjacency list to store connections\n",
    "adjacency_list = defaultdict(list)\n",
    "\n",
    "# Simulate connections using a normal distribution (15-20 connections per person)\n",
    "np.random.seed(42)\n",
    "connections_per_person = np.random.normal(loc=17.5, scale=2, size=num_people).astype(int)\n",
    "connections_per_person = np.clip(connections_per_person, 15, 20)  # Ensure between 15 and 20 connections\n",
    "\n",
    "# Generate adjacency lists\n",
    "for i, person in data.iterrows():\n",
    "    connections = np.random.choice(range(num_people), size=connections_per_person[i], replace=False)\n",
    "    for connection in connections:\n",
    "        if connection != i:  # Avoid self-loops\n",
    "            adjacency_list[person['LinkedIn Name']].append(data.iloc[connection]['LinkedIn Name'])\n",
    "\n",
    "# Display a sample of the adjacency list\n",
    "sample_adjacency_list = {k: adjacency_list[k] for k in list(adjacency_list.keys())[:3]}\n",
    "\n",
    "print(\"Sample Adjacency List:\")\n",
    "for name, connections in sample_adjacency_list.items():\n",
    "    print(f\"{name} -> {', '.join(connections)}\")\n",
    "\n",
    "# Save the adjacency list to a file (if needed)\n",
    "# For example, saving as JSON for further analysis\n",
    "import json\n",
    "with open('./adjacency_list.json', 'w') as f:\n",
    "    json.dump(adjacency_list, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DFS\n",
    "\n",
    "def dfs_with_path(start, key, visited=None, path=None):\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "    if path is None:\n",
    "        path = [start]\n",
    "\n",
    "    visited.add(start)\n",
    "\n",
    "    # Check if the current person is the target\n",
    "    if start == key:\n",
    "        return path  # Return the path to the target person\n",
    "\n",
    "    # Perform DFS on the neighbors\n",
    "    for neighbor, _ in adjacency_list.get(start, []):\n",
    "        if neighbor not in visited:\n",
    "            result = dfs_with_path(neighbor, key, visited, path + [neighbor])\n",
    "            if result:\n",
    "                return result  # Return the path as soon as a person with the required profession is found\n",
    "\n",
    "    return None  # No person with the required profession found in the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "def bfs_with_path(start, target):\n",
    "    # Queue for BFS (stores (name, path))\n",
    "    queue = deque([(start, [start])])\n",
    "    visited = set()\n",
    "    visited.add(start)\n",
    "\n",
    "    while queue:\n",
    "        current, path = queue.popleft()\n",
    "\n",
    "        # Check if the current person is the target\n",
    "        if current == target:\n",
    "            return path  # Return the path to the target person\n",
    "\n",
    "        # Explore the neighbors (connections)\n",
    "        for neighbor in adjacency_list.get(current, []):  # No unpacking needed\n",
    "            if neighbor not in visited:\n",
    "                visited.add(neighbor)\n",
    "                queue.append((neighbor, path + [neighbor]))\n",
    "\n",
    "    return None  # No path to the target found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saurabh Gupta -> Rachna Sharma -> Mohammed Irfan -> Roopesh Rai\n"
     ]
    }
   ],
   "source": [
    "starting_name = \"Saurabh Gupta\"\n",
    "target_name = \"Roopesh Rai\"  # The target person's name\n",
    "\n",
    "# Perform BFS to find a path from starting_name to target_name\n",
    "path = bfs_with_path(starting_name, target_name)\n",
    "\n",
    "if path:\n",
    "    print(\" -> \".join(path))  # Print the path if found\n",
    "else:\n",
    "    print(f\"No person with the name {target_name} found in the network starting from {starting_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saurabh Gupta -> Rachna Sharma -> Mohammed Irfan -> Roopesh Rai\n",
      "\n",
      "Corpus data for selected person:\n",
      "Roopesh Rai is the Founder Chief Executive Officer of Bakri Chhap Agro Tourism Natural Products Pvt Ltd. BakriChhap pioneering initiative seeks transform rural India sustainable tourism empowerment local communities. Roopesh sees potential rural India offer real reality hospitality tourism. He understands youth rural areas hold key change problem solution fostering eco entrepreneurship.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "starting_name = \"Saurabh Gupta\"\n",
    "target = \"Roopesh Rai\"\n",
    "\n",
    "# Perform BFS to find a path from starting_name to target_name\n",
    "path = bfs_with_path(starting_name, target)\n",
    "\n",
    "if path:\n",
    "    print(\" -> \".join(path))  # Print the path\n",
    "\n",
    "    # Extract the last person (the one with the required profession) from the path\n",
    "    selected_person = path[-1]\n",
    "\n",
    "    # Load the summarized corpus data with UTF-8 encoding\n",
    "    file_path = './summarized_corpus.csv'  # Path to the summarized corpus file\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        corpus_data = f.readlines()\n",
    "\n",
    "    # Find the row with the selected person\n",
    "    person_data = None\n",
    "    for line in corpus_data:\n",
    "        name, data = line.split(\",\", 1)  # Split the line by the first comma\n",
    "        if name.strip() == selected_person:  # Compare the name with the selected person\n",
    "            person_data = data.strip()  # Get the corpus data (remove leading/trailing spaces)\n",
    "            break\n",
    "\n",
    "    if person_data:\n",
    "        print(\"\\nCorpus data for selected person:\")\n",
    "        print(person_data)\n",
    "    else:\n",
    "        print(f\"No corpus data found for {selected_person}\")\n",
    "else:\n",
    "    print(f\"{target} not found in the network starting from {starting_name}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
