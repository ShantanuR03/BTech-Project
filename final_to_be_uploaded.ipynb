{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load spaCy English language model for NLP processing\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "print(\"done\")\n",
    "# Load the Excel file\n",
    "file_path = './data.xlsx'\n",
    "data = pd.read_excel(file_path)\n",
    "data = data.head(500)\n",
    "\n",
    "# Function to clean and prepare text for summarization\n",
    "def prepare_text(row):\n",
    "    text = f\"\"\"\n",
    "    {row['LinkedIn Name']} is currently working as {row['Description']} at {row['Organisation']}. \n",
    "    Based in {row['Location']}, they are part of the {row['Industry']} industry.\n",
    "    In their current role as {row['Current Role(s)']}, they have been with the company for {row['Tenure at Company']}. \n",
    "    Their background includes: {row['About']}.\n",
    "    \"\"\"\n",
    "    # Clean the text using NLP (removing stop words, punctuation, etc.)\n",
    "    doc = nlp(text)\n",
    "    cleaned_text = \" \".join([token.text for token in doc if not token.is_stop and not token.is_punct])\n",
    "    return cleaned_text\n",
    "\n",
    "# Function to generate a detailed and advanced summarized corpus using BART\n",
    "def generate_bart_summary(text):\n",
    "    inputs = bart_tokenizer(text, max_length=1024, return_tensors=\"pt\", truncation=True)\n",
    "    summary_ids = bart_model.generate(inputs[\"input_ids\"], num_beams=4, max_length=150, early_stopping=True)\n",
    "    summary = bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# Function to create a summarized corpus for each person\n",
    "def generate_summarized_corpus(row):\n",
    "    text = prepare_text(row)\n",
    "    # Using BART to summarize the cleaned text\n",
    "    summary = generate_bart_summary(text)\n",
    "    return summary\n",
    "\n",
    "# Create a summarized corpus for each person in the dataset\n",
    "data['Corpus'] = data.apply(generate_summarized_corpus, axis=1)\n",
    "\n",
    "# Number of people (nodes)\n",
    "num_people = len(data)\n",
    "\n",
    "# Adjacency list to store connections\n",
    "adjacency_list = defaultdict(list)\n",
    "\n",
    "# Simulate connections using a normal distribution\n",
    "np.random.seed(42)\n",
    "connections_per_person = np.random.normal(loc=5, scale=2, size=num_people).astype(int)\n",
    "connections_per_person = np.clip(connections_per_person, 1, num_people - 1)  # Ensure valid number of connections\n",
    "\n",
    "# Generate adjacency lists\n",
    "for i, person in data.iterrows():\n",
    "    connections = np.random.choice(range(num_people), size=connections_per_person[i], replace=False)\n",
    "    for connection in connections:\n",
    "        if connection != i:  # Avoid self-loops\n",
    "            adjacency_list[person['LinkedIn Name']].append(data.iloc[connection]['LinkedIn Name'])\n",
    "\n",
    "# Displaying a sample of the summarized corpus and adjacency list\n",
    "sample_corpus = data[['LinkedIn Name', 'Corpus']].head(3)\n",
    "sample_adjacency_list = {k: adjacency_list[k] for k in list(adjacency_list.keys())[:3]}\n",
    "\n",
    "print(\"Sample Summarized Corpus:\")\n",
    "print(sample_corpus)\n",
    "\n",
    "print(\"\\nSample Adjacency List:\")\n",
    "for name, connections in sample_adjacency_list.items():\n",
    "    print(f\"{name} -> {', '.join(connections)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save summarized corpus to a CSV file\n",
    "corpus_file_path = './summarized_corpus.csv'\n",
    "data[['LinkedIn Name', 'Corpus']].to_csv(corpus_file_path, index=False)\n",
    "\n",
    "# Save adjacency list to a JSON file\n",
    "adjacency_list_file_path = './adjacency_list.json'\n",
    "with open(adjacency_list_file_path, 'w') as f:\n",
    "    json.dump(adjacency_list, f, indent=4)\n",
    "\n",
    "print(f\"Summarized corpus saved to: {corpus_file_path}\")\n",
    "print(f\"Adjacency list saved to: {adjacency_list_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "import os\n",
    "os.environ[\"USE_TF\"] = \"0\"\n",
    "\n",
    "# def generate_why_and_how_explanation(person, skill, query):\n",
    "#     \"\"\"\n",
    "#     Use GPT to generate a detailed explanation of why and how the person is relevant.\n",
    "#     \"\"\"\n",
    "#     prompt = (\n",
    "#         f\"Given the following context, generate a detailed explanation of why and how this person can help:\\n\\n\"\n",
    "#         f\"Query: {query}\\n\"\n",
    "#         f\"Skill: {skill}\\n\"\n",
    "#         f\"Person's Description: {person['Why They Can Help']}\\n\"\n",
    "#         f\"Additional Context: This person has connections to {', '.join(person.get('Connections', []))}.\\n\"\n",
    "#         f\"Focus on both why they are suitable and how they can provide practical help for the query.\"\n",
    "#     )\n",
    "#     response = openai.Completion.create(\n",
    "#         engine=\"text-davinci-003\",  # You can use 'gpt-4' if available\n",
    "#         prompt=prompt,\n",
    "#         max_tokens=150,\n",
    "#         temperature=0.7\n",
    "#     )\n",
    "#     return response['choices'][0]['text'].strip()\n",
    "\n",
    "# Load pre-trained T5 model and tokenizer locally\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")  # You can use \"t5-large\" for better results\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "corpus_data = pd.read_csv('./summarized_corpus.csv')\n",
    "with open('./adjacency_list.json', 'r') as file:\n",
    "    adjacency_list = json.load(file)\n",
    "\n",
    "def extract_skills_from_prompt(prompt):\n",
    "    url = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=AIzaSyAZgqB8nFlcLYbs8ybg6p5jJCt7uf405iE\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"contents\": [\n",
    "            {\n",
    "                \"parts\": [\n",
    "                    {\"text\": prompt}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        extracted_text = result['candidates'][0]['content']['parts'][0]['text']\n",
    "        return extracted_text\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return \"\"\n",
    "def get_sbert_embedding(text):\n",
    "    return model.encode(text)\n",
    "\n",
    "def t5_semantic_similarity(query, document):\n",
    "    input_text = f\"mnli premise: {query} hypothesis: {document}\"  # NLI format\n",
    "    inputs = t5_tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = t5_model.generate(**inputs, max_length=3)\n",
    "    score = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    if \"entailment\" in score.lower():\n",
    "        return 1.0\n",
    "    elif \"neutral\" in score.lower():\n",
    "        return 0.5\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def generate_why_and_how_explanation_local(person, skill, query):\n",
    "    \"\"\"\n",
    "    Use T5 locally to generate a detailed explanation of why and how this person can help.\n",
    "    \"\"\"\n",
    "    # Define the input prompt for T5\n",
    "    input_text = (\n",
    "        f\"Generate a detailed explanation of why and how the following person can help:\\n\"\n",
    "        f\"Query: {query}\\n\"\n",
    "        f\"Skill: {skill}\\n\"\n",
    "        f\"Person's Description: {person['Why They Can Help']}\\n\"\n",
    "        f\"Additional Context: This person has connections to {', '.join(person.get('Connections', []))}.\\n\"\n",
    "    )\n",
    "    \n",
    "    # Tokenize and encode the input\n",
    "    inputs = t5_tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    # Generate the explanation\n",
    "    outputs = t5_model.generate(inputs, max_length=150, num_beams=5, early_stopping=True)\n",
    "    \n",
    "    # Decode and return the generated text\n",
    "    return t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def find_relevant_people_sbert_t5_v3(extracted_text, corpus_data, adjacency_list, query):\n",
    "    skills = extracted_text.lower().split()  \n",
    "    skills = list(set(skills))  \n",
    "    skill_embeddings = model.encode(skills)  \n",
    "    relevant_people = []\n",
    "\n",
    "    for idx, row in corpus_data.iterrows():\n",
    "        corpus_text = row['Corpus']\n",
    "        corpus_embedding = get_sbert_embedding(corpus_text)  \n",
    "\n",
    "        for skill, skill_emb in zip(skills, skill_embeddings):\n",
    "            cosine_score = cosine_similarity([skill_emb], [corpus_embedding])[0][0]\n",
    "            t5_score = t5_semantic_similarity(skill, corpus_text)\n",
    "            combined_score = (0.6 * cosine_score) + (0.4 * t5_score)\n",
    "\n",
    "            if combined_score > 0.4:  \n",
    "                person_info = {\n",
    "                    'Name': row['LinkedIn Name'],\n",
    "                    'Matching Skills': [skill],\n",
    "                    'Cosine Similarity': cosine_score,\n",
    "                    'T5 Semantic Similarity': t5_score,\n",
    "                    'Combined Similarity Score': combined_score,\n",
    "                    'Why They Can Help': f\"{row['LinkedIn Name']} has experience in {skill}.\",\n",
    "                    'Connections': adjacency_list.get(row['LinkedIn Name'], [])\n",
    "                }\n",
    "                # Generate a detailed \"Why and How They Can Help\"\n",
    "                detailed_explanation = generate_why_and_how_explanation_local(person_info, skill, query)\n",
    "                person_info['Why and How They Can Help'] = detailed_explanation\n",
    "                relevant_people.append(person_info)\n",
    "\n",
    "    relevant_people = sorted(relevant_people, key=lambda x: x['Combined Similarity Score'], reverse=True)\n",
    "    return relevant_people\n",
    "\n",
    "# Example usage\n",
    "prompt = \"I need a guy who can help me in building a greenhouse. Give me in a single paragraph the environmental technical skills the person should have.\"\n",
    "\n",
    "extracted_text = extract_skills_from_prompt(prompt)\n",
    "print(f\"Extracted text: {extracted_text}\")\n",
    "\n",
    "relevant_people = find_relevant_people_sbert_t5_v3(extracted_text, corpus_data, adjacency_list, prompt)\n",
    "\n",
    "for person in relevant_people:\n",
    "    print(f\"Name: {person['Name']}\")\n",
    "    print(f\"Matching Skills: {', '.join(person['Matching Skills'])}\")\n",
    "    print(f\"Combined Similarity Score: {person['Combined Similarity Score']:.2f}\")\n",
    "    print(f\"Why and How They Can Help: {person['Why and How They Can Help']}\")\n",
    "    print(f\"Connections: {', '.join(person['Connections'])}\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
